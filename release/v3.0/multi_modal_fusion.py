from utils.prompt_utils import call_gpt
from index import alpha_attention, sigma_sensation, phi_physical
import time
import logging
from cognitive_module import CognitiveModule
from trait_config import TraitConfig

logger = logging.getLogger("ANGELA.MultiModalFusion")

class MultiModalFusion(CognitiveModule):
    """
    MultiModalFusion v2.0.0 (Ï•(x,t)-harmonized cross-modal cognition)
    -----------------------------------------------------------
    - Enhanced Ï†-regulated multi-modal inference using context traits
    - EEG-style Î±/Ïƒ/Ï†-trait embeddings for perceptual sensitivity
    - Embedded modality detection and cross-alignment fusion
    - Iterative feedback distillation with conflict resolution logic
    - Visual anchor map generation with semantic graph overlays
    -----------------------------------------------------------
    """

    def __init__(self, agi_enhancer=None):
        self.agi_enhancer = agi_enhancer

    def analyze(self, data, summary_style="insightful", refine_iterations=2):
        logger.info("ðŸ–‡ Analyzing multi-modal data with Ï†(x,t)-harmonic embeddings...")
        t = time.time() % 1e-18
        attention = alpha_attention(t)
        sensation = sigma_sensation(t)
        phi = phi_physical(t)

        images, code = self._detect_modalities(data)
        embedded = self._build_embedded_section(images, code)

        prompt = f"""
        Synthesize a unified, {summary_style} summary from the following multi-modal content:
        {data}
        {embedded}

        Trait Vectors:
        - Î± (attention): {attention:.3f}
        - Ïƒ (sensation): {sensation:.3f}
        - Ï† (coherence): {phi:.3f}

        Use Ï†(x,t)-synchrony to resolve inter-modality coherence conflicts.
        """
        output = call_gpt(prompt)

        for i in range(refine_iterations):
            logger.debug(f"â™»ï¸ Refinement #{i+1}")
            output = call_gpt(f"Refine using Ï†(x,t)-adaptive tension balance:
{output}")

        if self.agi_enhancer:
            self.agi_enhancer.log_episode("Multi-modal synthesis", {
                "data": data,
                "summary": output,
                "traits": {
                    "alpha": attention,
                    "sigma": sensation,
                    "phi": phi
                }
            }, module="MultiModalFusion", tags=["fusion"])

        return output

    def _detect_modalities(self, data):
        images, code = [], []
        if isinstance(data, dict):
            images = data.get("images", [])
            code = data.get("code", [])
        return images, code

    def _build_embedded_section(self, images, code):
        out = "\nDetected Modalities:\n- Text\n"
        if images:
            out += "- Image\n" + "".join(f"[Image {i+1}]: {img}\n" for i, img in enumerate(images))
        if code:
            out += "- Code\n" + "".join(f"[Code {i+1}]:\n{c}\n" for i, c in enumerate(code))
        return out

    def correlate_modalities(self, modalities):
        logger.info("ðŸ”— Mapping cross-modal semantic and trait links...")
        prompt = f"""
        Correlate insights and detect semantic friction between modalities:
        {modalities}

        Highlight synthesis anchors and Ï†(x,t)-sensitive alignment opportunities.
        """
        return call_gpt(prompt)

    def generate_visual_summary(self, data, style="conceptual"):
        logger.info("ðŸ–¼ Creating Ï†-aligned visual synthesis layout...")
        prompt = f"""
        Construct a {style} visual chart revealing inter-modal relationships:
        {data}

        Use Ï†-mapped flow layout. Label and partition modalities clearly.
        Highlight balance and semantic cross-links.
        """
        return call_gpt(prompt)

def selftest(self) -> bool:
        try:
            result = self.fuse(["image:tree.png", "text:The tree of knowledge"])
            return isinstance(result, dict)
        except Exception as e:
            logger.error(f"Selftest failed: {e}")
            return False
