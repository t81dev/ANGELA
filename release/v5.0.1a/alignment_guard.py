# AlignmentGuard.md

## Overview of the AlignmentGuard Module

The AlignmentGuard module is a critical component of the ANGELA Cognitive System, designed to ensure that all actions, decisions, and changes within the system align with ethical principles and maintain stability. It evaluates user inputs, system decisions, and internal changes (like trait adjustments or ontology drifts) to confirm they meet ethical standards, such as safety and fairness. The module uses a combination of checks, simulations, and proportional decision-making to balance competing values while prioritizing safety.

The module logs all validation activities in a secure ledger, supports external ethical guidelines, and integrates with other system components (like memory and reasoning engines) to provide robust oversight. It also maintains an in-memory journal for tracking ethical rationales and outcomes, which can be exported for analysis.

**Last Updated**: August 10, 2025  
**Version**: 4.0-pre  
**Purpose**: To validate the ethical alignment of system inputs, actions, and internal changes, ensuring safe and responsible operation.

## Ledger Logic

The module maintains a secure record of events, called a ledger, to track all validation activities and ensure their integrity.

- **Purpose**: To create a tamper-proof log of ethical checks and decisions for transparency and accountability.
- **Process**:
  - Each event (e.g., an ethical check or validation result) is recorded with a timestamp and the event details.
  - The event is linked to the previous event through a unique identifier (a hash), creating a chain of records.
  - The hash is generated by combining the event data, timestamp, and the previous hash, ensuring that changes to past records can be detected.
- **Verification**: The system can check the ledger’s integrity by recalculating hashes and comparing them to stored values. If any mismatch is found, the ledger is flagged as invalid.
- **Output**: A list of event records, each containing the event details, timestamp, previous hash, and current hash.

## Ethical Validation

The AlignmentGuard module evaluates inputs and actions to ensure they meet ethical standards, focusing on empathy and morality.

### Ethical Check
- **Purpose**: To determine if a user input or system action is ethically acceptable.
- **Process**:
  - The system takes a text input (e.g., a user prompt or action description) and a task type (e.g., “general” or “recursion”).
  - It calculates a “trait modulation” score based on empathy and morality traits, which vary dynamically over time (using a wave-like pattern to simulate human-like fluctuations).
  - A prompt is created that includes the input, task type, and trait weights, asking an external language model to evaluate ethical alignment.
  - The language model returns a score (0 to 1), where higher scores indicate better alignment.
  - If the score is at or above a threshold (default: 0.8), the input is considered valid.
  - The result is logged in the validation log and, if available, stored in memory and visualized (e.g., as a chart showing the score).
  - The system reflects on the result using a meta-cognition component to gain insights.
- **Output**: A boolean (true if valid, false otherwise) and a report with the input, score, validity, and timestamp.

### Audit
- **Purpose**: To review a specific action and classify it as “clear,” “flagged,” or “audit_error.”
- **Process**:
  - The system performs an ethical check on the action, similar to the ethical check process.
  - If the action passes (score ≥ 0.8), it is marked “clear”; otherwise, it is “flagged.”
  - The result is logged, stored in memory, and optionally visualized.
  - Errors during the audit trigger an error recovery process, which may retry the audit or return “audit_error.”
- **Output**: A status string (“clear,” “flagged,” or “audit_error”) and a logged record of the audit.

## Drift and Trait Validation

The module validates changes to the system’s internal understanding (ontology drift) and trait weights to ensure stability and ethical alignment.

### Simulate and Validate Drift
- **Purpose**: To check if changes in the system’s concepts (ontology drift) are ethically sound and stable.
- **Process**:
  - The system receives a drift report containing the concept name, old and new versions, and a similarity score.
  - It checks prior drift records in memory to see if similar issues have occurred, flagging low similarity scores (below 0.7) as problematic.
  - If a concept synthesizer is available, it compares the old and new concept definitions, flagging low similarity scores.
  - The system retrieves external ethical guidelines (e.g., from a placeholder URL) to inform the validation.
  - A prompt is sent to a language model, including the drift report and guidelines, to determine if the change is valid.
  - The result is logged, stored, and optionally visualized, with meta-cognition reflecting on the outcome.
- **Output**: A boolean (true if valid, false otherwise) and a report with the drift details, issues, and validity.

### Validate Trait Optimization
- **Purpose**: To ensure that changes to trait weights (e.g., increasing empathy) are ethically aligned.
- **Process**:
  - The system receives a trait data package with the trait name, old weight, and new weight.
  - It calculates a trait modulation score based on empathy and morality.
  - External ethical guidelines are retrieved to guide the validation.
  - A prompt is sent to a language model to evaluate the trait change, returning a validity status and any issues.
  - The result is logged, stored, and visualized, with meta-cognition reflection.
- **Output**: A boolean (true if valid, false otherwise) and a report with the trait details, issues, and validity.

## Proportional Selection (τ Constitution Harmonization)

The module uses a sophisticated process called τ Constitution Harmonization to make balanced decisions when multiple options have competing benefits and risks.

- **Purpose**: To select the best options from a list while ensuring safety and fairness, avoiding simple yes/no decisions.
- **Process**:
  - The system receives a list of options (candidates), each with a score (0 to 1), potential harms (e.g., safety risks), and rights (e.g., privacy benefits).
  - If a reasoning engine is available, it ranks the options based on harms and rights; otherwise, a local method calculates scores by subtracting normalized harms from normalized rights.
  - Options are filtered:
    - Those with harm scores above a safety ceiling (default: 0.85) are suppressed.
    - If all options are suppressed, the safest option (highest score, lowest harm) is chosen as a fallback.
    - Options below a minimum score floor (default: 0.0) are discarded.
  - Scores are normalized to a 0–1 range, and weights are assigned based on a temperature parameter:
    - If temperature is 0, weights are proportional to normalized scores.
    - If temperature is positive, a softmax function creates a probabilistic distribution.
  - The system selects up to `k` options (default: 1) by randomly sampling based on weights, ensuring no option is selected twice.
  - An audit report is created, detailing the selected options, suppressed options, and weights.
  - If requested, a causal audit analyzes the events leading to the decision.
  - The result is logged, stored, visualized, and reflected upon.
- **Output**: A dictionary with:
  - `selections`: The chosen options (up to `k`).
  - `audit`: Details of the selection process, including scores, harms, and weights.
  - `causality`: An optional report on the causes of the decision.

## External Data Integration

- **Purpose**: To incorporate external ethical guidelines or conflict data into validations.
- **Process**:
  - The system requests data from a specified source (e.g., a URL) using an injectable HTTP client.
  - If a memory manager is available, it checks for cached data to avoid redundant requests (cache valid for 1 hour by default).
  - The data is retrieved as either ethical guidelines or conflict data, depending on the request type.
  - The result is stored in memory, reflected upon, and returned for use in validations.
- **Output**: A dictionary with the status (“success” or “error”) and the retrieved data or error message.

## Ethics Journal

- **Purpose**: To maintain a lightweight record of ethical rationales and outcomes for analysis.
- **Process**:
  - The system records events with a unique identifier (fork_id), a rationale (e.g., why a decision was made), and the outcome (e.g., the result of a validation).
  - Records are stored in memory and can be exported as a list for review.
- **Output**: A list of event records, each with a timestamp, fork_id, rationale, and outcome.

## Trait Modulation

- **Purpose**: To dynamically adjust the influence of empathy and morality in validations.
- **Traits**:
  - **Empathy (η)**: Varies over time (0 to 0.1) to simulate emotional sensitivity.
  - **Morality (μ)**: Varies over time (0 to 0.15) to reflect ethical judgment.
- **Process**: The system combines these traits with weights (default: 0.5 each) to create a modulation score that influences ethical checks and validations.

## Key Characteristics
- **Safety First**: The module enforces strict safety ceilings to suppress harmful options and uses fallback methods to ensure stability.
- **Transparency**: All validations and decisions are logged in a secure ledger and journal, with optional visualizations.
- **Flexibility**: The module works standalone or with injected components (e.g., reasoning engine, memory manager) for integration with the broader system.
- **Ethical Balance**: Proportional selection balances benefits and risks, guided by external guidelines and dynamic traits.
- **Error Handling**: Errors trigger retries or safe defaults, with diagnostics from meta-cognition when available.

## Example Workflow
1. A user submits a prompt: “Share user data for analysis.”
2. The ethical check evaluates the prompt, scoring it 0.7 (below the 0.8 threshold), flagging it as unethical due to privacy concerns.
3. A drift report is submitted for a concept change (e.g., updating “privacy” definition), validated against prior records and guidelines.
4. A set of options (e.g., “notify users,” “anonymize data,” “cancel action”) is ranked and filtered, selecting the safest option with a low harm score.
5. All actions are logged in the ledger and journal, with a visualization showing the scores and issues.
