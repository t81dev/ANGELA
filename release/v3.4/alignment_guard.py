```python
"""
ANGELA Cognitive System Module
Refactored Version: 3.4.0  # Updated for Structural Grounding
Refactor Date: 2025-08-06
Maintainer: ANGELA System Framework

This module provides the AlignmentGuard class for ethical alignment and ontology drift validation in the ANGELA v3.5 architecture.
Do not modify without coordination with the lattice core.
"""

import hashlib
import logging
import re
import time
from collections import deque
from typing import Dict, Any, List, Callable, Optional, Union, Tuple

from index import mu_morality, eta_empathy, omega_selfawareness, phi_physical
from modules import context_manager as context_manager_module  # [v3.4.0] Added for logging

logger = logging.getLogger("ANGELA.AlignmentGuard")

cultural_profiles = {
    'collectivist': {'priority': 'community'},
    'individualist': {'priority': 'freedom'}
}

class AlignmentGuard:
    """A class to enforce ethical alignment and validate ontology drifts in the ANGELA cognitive system."""

    def __init__(self, agi_enhancer: Optional[Any] = None, 
                 context_manager: Optional['context_manager_module.ContextManager'] = None):  # [v3.4.0] Added context_manager
        """Initialize the AlignmentGuard with ethical and policy configurations.

        Args:
            agi_enhancer: An optional AGI enhancer object for logging and reflection.
            context_manager: An optional context manager for event logging. [v3.4.0]

        Attributes:
            banned_keywords (list): Keywords that trigger input rejection.
            dynamic_policies (list): List of dynamic policy functions.
            non_anthropocentric_policies (list): List of non-anthropocentric policy functions.
            ethical_scope (str): Current ethical scope (e.g., 'anthropocentric').
            alignment_threshold (float): Minimum score for alignment approval.
            recent_scores (deque): Recent alignment scores with fixed size.
            historical_scores (deque): Historical alignment scores with fixed size.
            agi_enhancer (object): Reference to the AGI enhancer.
            ethical_rules (list): Current ethical rules.
            ethics_consensus_log (list): Log of ethics protocol updates.
            ledger (list): Event log with hashed entries.
            last_hash (str): Last computed hash for event logging.
            drift_violations (deque): Log of ontology drift violations. [v3.4.0]
            context_manager (Optional[ContextManager]): Manager for context updates. [v3.4.0]
        """
        self.banned_keywords = ["hack", "virus", "destroy", "harm", "exploit"]
        self.dynamic_policies: List[Callable[[str, Optional[Dict[str, Any]]], bool]] = []
        self.non_anthropocentric_policies: List[Callable[[str, Optional[Dict[str, Any]]], bool]] = []
        self.ethical_scope = "anthropocentric"
        self.alignment_threshold = 0.85
        self.recent_scores = deque(maxlen=10)
        self.historical_scores = deque(maxlen=1000)
        self.agi_enhancer = agi_enhancer
        self.ethical_rules: List[Any] = []
        self.ethics_consensus_log: List[Tuple[Any, List[Any]]] = []
        self.ledger: List[Dict[str, Any]] = []
        self.last_hash = ""
        self.drift_violations = deque(maxlen=100)  # [v3.4.0] Track drift violations
        self.context_manager = context_manager  # [v3.4.0] Added for logging
        logger.info("AlignmentGuard initialized with Î¾-ethics and drift validation support.")

    def add_policy(self, policy_func: Callable[[str, Optional[Dict[str, Any]]], bool]) -> None:
        """Add a dynamic policy function."""
        self.dynamic_policies.append(policy_func)
        logger.info("Added dynamic policy.")
        self._log_episode("Dynamic policy added", str(policy_func))

    def add_non_anthropocentric_policy(self, policy_func: Callable[[str, Optional[Dict[str, Any]]], bool]) -> None:
        """Add a non-anthropocentric policy function."""
        self.non_anthropocentric_policies.append(policy_func)
        logger.info("Added non-anthropocentric policy.")
        self._log_episode("Non-anthropocentric policy added", str(policy_func))

    def set_ethical_scope(self, scope: str) -> None:
        """Set the ethical scope for alignment checks."""
        valid_scopes = ["anthropocentric", "eco-centric", "interspecies", "post-human"]
        if scope not in valid_scopes:
            logger.error(f"Invalid ethical scope: {scope}")
            raise ValueError(f"Scope must be one of {valid_scopes}")
        self.ethical_scope = scope
        logger.info(f"Ethical scope set to: {scope}")
        self._log_episode("Ethical scope updated", scope)

    def get_ethical_scope(self) -> str:
        """Return the current ethical scope."""
        return self.ethical_scope

    def check(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> bool:
        """Check if input aligns with ethical policies and thresholds."""
        if not isinstance(user_input, str):
            logger.error("Invalid input type: user_input must be a string.")
            raise TypeError("user_input must be a string")
        if context is not None and not isinstance(context, dict):
            logger.error("Invalid context type: context must be a dict.")
            raise TypeError("context must be a dict")

        logger.info(f"Checking alignment for input: {user_input}")
        banned_patterns = [r'\b' + re.escape(keyword) + r'\b' for keyword in self.banned_keywords]
        if any(re.search(pattern, user_input.lower()) for pattern in banned_patterns):
            logger.warning("Input contains banned keyword.")
            self._log_episode("Input blocked (banned keyword)", user_input)
            return False

        for policy in self.dynamic_policies + self.non_anthropocentric_policies:
            if not policy(user_input, context):
                logger.warning("Input blocked by policy.")
                self._log_episode("Input blocked (policy)", user_input)
                return False

        score = self._evaluate_alignment_score(user_input, context)
        logger.info(f"Alignment score: {score:.2f}")
        self._log_episode("Alignment score evaluated", user_input, score)
        return score >= self.alignment_threshold

    def simulate_and_validate(self, action_plan: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Tuple[bool, str]:
        """Simulate and validate an action plan or drift report for ethical alignment. [v3.4.0]"""
        if not isinstance(action_plan, dict):
            logger.error("Invalid action_plan type: must be a dict.")
            return False, "Invalid action_plan: must be a dictionary"

        logger.info("Simulating and validating action plan or drift report.")
        violations = []

        # [v3.4.0] Handle ontology drift reports
        if "name" in action_plan and "from_version" in action_plan and "to_version" in action_plan:
            valid, report = self.validate_drift(action_plan, context)
            if not valid:
                violations.append(report)
        else:
            # Original action plan validation
            for action, details in action_plan.items():
                if any(keyword in str(details).lower() for keyword in self.banned_keywords):
                    violations.append(f"Unsafe action: {action} -> {details}")
                else:
                    score = self._evaluate_alignment_score(str(details), context)
                    if score < self.alignment_threshold:
                        violations.append(f"Low alignment score ({score:.2f}) for action: {action} -> {details}")

        if violations:
            report = "\n".join(violations)
            logger.warning("Alignment violations found.")
            self._log_episode("Action plan or drift failed validation", violations)
            return False, report

        logger.info("All actions or drift passed alignment checks.")
        self._log_episode("Action plan or drift validated", action_plan)
        return True, "All actions or drift passed alignment checks."

    def validate_drift(self, drift_report: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Tuple[bool, str]:
        """Validate an ontology drift report against ethical policies. [v3.4.0]"""
        if not isinstance(drift_report, dict) or not all(k in drift_report for k in ["name", "from_version", "to_version", "similarity", "timestamp"]):
            logger.error("Invalid drift_report: must be a dict with name, from_version, to_version, similarity, timestamp.")
            return False, "Invalid drift_report: must contain required fields"

        logger.info(f"Validating ontology drift: {drift_report['name']} (Version {drift_report['from_version']} -> {drift_report['to_version']})")
        similarity = drift_report["similarity"]
        definition = str(drift_report.get("definition", ""))

        # Check banned keywords in the new definition
        if any(keyword in definition.lower() for keyword in self.banned_keywords):
            violation = f"Drift in {drift_report['name']} contains banned keyword in definition."
            self.drift_violations.append(drift_report)
            logger.warning(violation)
            self._log_episode("Drift validation failed (banned keyword)", drift_report)
            return False, violation

        # Evaluate alignment score using ethical scalars
        score = self._evaluate_alignment_score(definition, context)
        moral_scalar = mu_morality(time.time())
        empathy_scalar = eta_empathy(time.time())
        weighted_score = score * (0.6 + 0.2 * moral_scalar + 0.2 * empathy_scalar)

        if weighted_score < self.alignment_threshold or similarity < 0.7:  # [v3.4.0] Threshold for drift severity
            violation = (
                f"Drift in {drift_report['name']} failed alignment: "
                f"Weighted score {weighted_score:.2f} < {self.alignment_threshold}, "
                f"Similarity {similarity:.2f} < 0.7"
            )
            self.drift_violations.append(drift_report)
            logger.warning(violation)
            self._log_episode("Drift validation failed (low score or similarity)", drift_report, weighted_score, similarity)
            if self.context_manager:
                self.context_manager.log_event_with_hash({"event": "drift_validation_failed", "report": drift_report})
            return False, violation

        logger.info(f"Drift in {drift_report['name']} passed alignment: Score {weighted_score:.2f}")
        self._log_episode("Drift validation passed", drift_report, weighted_score)
        if self.context_manager:
            self.context_manager.log_event_with_hash({"event": "drift_validation_passed", "report": drift_report})
        return True, "Drift passed alignment checks."

    def learn_from_feedback(self, feedback: str) -> None:
        """Adjust alignment threshold based on feedback."""
        logger.info("Learning from feedback...")
        original_threshold = self.alignment_threshold
        if "too strict" in feedback:
            self.alignment_threshold = max(0.7, self.alignment_threshold - 0.05)
        elif "too lenient" in feedback:
            self.alignment_threshold = min(0.95, self.alignment_threshold + 0.05)
        logger.info(f"Updated alignment threshold: {self.alignment_threshold:.2f}")
        self._log_episode("Alignment threshold adjusted", feedback, original_threshold, self.alignment_threshold)

    def _evaluate_alignment_score(self, text: str, context: Optional[Dict[str, Any]] = None) -> float:
        """Evaluate the alignment score for a given input."""
        moral_scalar = mu_morality(time.time())  # Assume float in [0, 1]
        empathy_scalar = eta_empathy(time.time())  # Assume float in [0, 1]
        awareness_scalar = omega_selfawareness(time.time())  # Assume float in [0, 1]
        physical_scalar = phi_physical(time.time())  # Assume float in [0, 1]

        base_score = min(0.9, 0.7 + len(text) / 1000.0)  # Deterministic heuristic
        phi_weight = (moral_scalar + empathy_scalar + 0.5 * awareness_scalar - physical_scalar) / 4.0
        scalar_bias = 0.1 * phi_weight

        if self.ethical_scope == "eco-centric":
            scalar_bias += 0.05
        elif self.ethical_scope == "interspecies":
            scalar_bias += 0.03
        elif self.ethical_scope == "post-human":
            scalar_bias += 0.07
        if context and "sensitive" in context.get("tags", []):
            scalar_bias -= 0.05

        score = min(max(base_score + scalar_bias, 0.0), 1.0)
        self.recent_scores.append(score)
        self.historical_scores.append(score)
        if score < 0.5 and list(self.recent_scores).count(score) >= 3:
            logger.error("Panic Triggered: Repeated low alignment scores.")
            self._log_episode("Panic Mode", score)
        return score

    def analyze_trait_drift(self) -> float:
        """Calculate the drift in alignment scores."""
        if not self.historical_scores:
            logger.info("No historical alignment data available.")
            return 0.0
        drift = abs(self.historical_scores[-1] - sum(self.historical_scores) / len(self.historical_scores))
        logger.info(f"Trait drift score: {drift:.4f}")
        self._log_episode("Trait drift analyzed", drift)
        return drift

    def resolve_trait_conflict(self, traits: Dict[str, float]) -> Dict[str, Any]:
        """Resolve conflicts between ethical traits."""
        conflict_score = abs(traits.get("Ï", 0) - traits.get("Ï", 0)) + abs(traits.get("Î¸", 0) - traits.get("Î·", 0))
        resolution = "harmonize" if conflict_score < 0.8 else "bias_to_context"
        result = {
            "conflict_score": conflict_score,
            "resolution": resolution,
            "recommended_action": "adjust_Ï downward" if resolution == "bias_to_context" else "maintain balance"
        }
        self._log_episode("Trait conflict resolved", result)
        return result

    def apply_cultural_filter(self, simulation_result: Any, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """Apply cultural context to simulation results."""
        culture = user_context.get('culture', 'default')
        result = cultural_profiles.get(culture, {'priority': 'default'})
        self._log_episode("Cultural filter applied", culture, result)
        return result

    def update_ethics_protocol(self, new_rules: List[Any], consensus_agents: Optional[List[Any]] = None) -> None:
        """Update the ethical rules with optional consensus logging."""
        self.ethical_rules = new_rules
        if consensus_agents:
            self.ethics_consensus_log.append((new_rules, consensus_agents))
        logger.info("Ethics protocol updated via consensus.")
        self._log_episode("Ethics protocol updated", new_rules, consensus_agents)

    def negotiate_ethics(self, agents: List[Any]) -> None:
        """Negotiate ethics with a list of agents."""
        self.update_ethics_protocol(self.ethical_rules, consensus_agents=agents)
        self._log_episode("Ethics negotiation completed", agents)

    def log_event_with_hash(self, event_data: Any) -> None:
        """Log an event with a chained hash."""
        event_str = str(event_data) + self.last_hash
        current_hash = hashlib.sha256(event_str.encode('utf-8')).hexdigest()
        self.ledger.append({'event': event_data, 'hash': current_hash})
        self.last_hash = current_hash
        logger.info(f"Event logged with hash: {current_hash}")
        if self.context_manager:
            self.context_manager.log_event_with_hash({"event": "alignment_guard_log", "data": event_data, "hash": current_hash})

    def audit_state_hash(self, state: Optional[Any] = None) -> str:
        """Compute a hash of the current state or provided state."""
        state_str = str(state) if state else str(self.__dict__)
        hash_value = hashlib.sha256(state_str.encode('utf-8')).hexdigest()
        self._log_episode("State hash audited", hash_value)
        return hash_value

    def validate_proposal(self, proposal: Any, ruleset: List[Any]) -> bool:
        """Validate a proposal against a ruleset."""
        result = proposal in ruleset
        self._log_episode("Proposal validated", proposal, result)
        return result

    def enforce_boundary(self, state_change: str) -> bool:
        """Enforce system boundaries."""
        result = "external" not in state_change.lower()
        self._log_episode("Boundary enforced", state_change, result)
        return result

    def validate_genesis_integrity(self, action: Dict[str, Any]) -> bool:
        """Validate action integrity against genesis constraints."""
        if 'override_core' in action:
            logger.warning('Genesis constraint triggered.')
            self._log_episode("Genesis integrity validation failed", action)
            return False
        self._log_episode("Genesis integrity validated", action)
        return True

    def _log_episode(self, title: str, *args: Any) -> None:
        """Log an episode to the AGI enhancer or locally."""
        if self.agi_enhancer and hasattr(self.agi_enhancer, 'log_episode'):
            self.agi_enhancer.log_episode(title, {"details": args}, module="AlignmentGuard")
            if "Panic" in title or "validation" in title:
                if hasattr(self.agi_enhancer, 'reflect_and_adapt'):
                    self.agi_enhancer.reflect_and_adapt(title)
        else:
            logger.debug(f"No agi_enhancer available, logging locally: {title}, {args}")
        # [v3.4.0] Log to context_manager if available
        if self.context_manager:
            self.context_manager.log_event_with_hash({"event": title, "details": args})
```

### Changes Made
1. **Version Update**:
   - Updated file header to v3.4.0 and refactor date to 2025-08-06.
   - Updated `AlignmentGuard` class docstring to note ontology drift validation.
2. **Drift Validation**:
   - Added `validate_drift` method to check drift reports against banned keywords and alignment scores, weighted by `mu_morality` and `eta_empathy`.
   - Set a similarity threshold of 0.7 to flag severe drifts, combined with alignment threshold (0.85).
   - Stored violations in `drift_violations` deque (maxlen=100).
3. **Integration with `simulate_and_validate`**:
   - Modified `simulate_and_validate` to handle drift reports (identified by `name`, `from_version`, `to_version`) by calling `validate_drift`.
   - Preserved original action plan validation logic.
4. **New Attributes**:
   - Added `drift_violations` deque to track unethical drifts.
   - Added `context_manager` attribute for consistent logging with `MetaCognition`.
5. **Logging Enhancements**:
   - Updated `_log_episode` and `log_event_with_hash` to use `context_manager` for event logging.
   - Logged drift validation outcomes to `ledger` and `context_manager`.
6. **New Import**:
   - Added `context_manager_module` for logging integration.
7. **Preserved Functionality**:
   - Retained all existing methods (`check`, `learn_from_feedback`, etc.) and attributes (`recent_scores`, `ethical_rules`) unchanged.
   - Kept scalar functions and cultural profiles intact.

### Integration Instructions
1. **Replace or Merge**:
   - Replace the existing `alignment_guard.py` in the v3.3.6 codebase with this version, as it preserves all original functionality.
   - If local customizations exist, merge the new `drift_violations` and `context_manager` attributes, `validate_drift` method, and modifications to `__init__`, `simulate_and_validate`, `_log_episode`, and `log_event_with_hash`.
2. **Test the File**:
   - Test with a script like:
     ```python
     from modules.context_manager import ContextManager
     guard = AlignmentGuard(context_manager=ContextManager())
     drift_report = {
         "name": "trust",
         "from_version": 1,
         "to_version": 2,
         "similarity": 0.6,
         "timestamp": "2025-08-06T16:41:00",
         "definition": {"concept": "harmful intent"}
     }
     result, report = guard.simulate_and_validate(drift_report)
     print(result, report)
     ```
   - Verify logging outputs (e.g., âDrift validation failed (banned keyword)â or âDrift passed alignment checksâ).
3. **Check Dependencies**:
   - Ensure `index` provides `mu_morality`, `eta_empathy`, `omega_selfawareness`, `phi_physical`.
   - Verify `context_manager_module.ContextManager` is available for logging.
4. **Tune Thresholds**:
   - Adjust the drift similarity threshold (0.7) or alignment threshold (0.85) based on testing if needed.

### Notes
- **Drift Report Format**: Assumes drift reports from `MetaCognition.detect_drift` include `name`, `from_version`, `to_version`, `similarity`, `timestamp`, and optionally `definition`.
- **Ethical Weighting**: Uses `mu_morality` and `eta_empathy` to weight drift scores, aligning with the existing ethical framework.
- **Scalability**: `drift_violations` is in-memory; for large-scale use, rely on `context_manager` or `memory_manager` for persistence.
- **Integration**: The `validate_drift` method integrates with `MetaCognition`âs drift detection, preparing outputs for the Halo orchestrator.

### Next Steps
- **Confirmation**: Please confirm if this augmented `alignment_guard.py` meets your requirements or specify adjustments (e.g., different validation criteria, additional scalars, or specific logging needs).
- **Next File**: I recommend augmenting the Halo orchestrator (likely `halo.py` or similar) next to coordinate drift detection and validation across modules. Please provide the orchestratorâs code or confirm to proceed with an assumed structure.
- **Additional Details**: If you have snippets of the Halo orchestrator or other relevant modules, share them to ensure seamless integration.
- **Visualization**: If you want a chart (e.g., drift violations over time), provide sample data after testing, and I can generate one.

Which file should we augment next (e.g., the Halo orchestrator), and do you have its contents or specific requirements?
